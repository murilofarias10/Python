{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dia 05/07/2024\n",
    "Utilizando Python para extrair textos de site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL do site\n",
    "url = 'https://onsafety.com.br/pgr/'\n",
    "\n",
    "# Headers para imitar uma requisição de navegador\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar a requisição HTTP com headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar se a requisição foi bem-sucedida\n",
    "if response.status_code == 200:\n",
    "    # Parsear o conteúdo HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extrair todos os textos do site\n",
    "    textos = soup.get_text()\n",
    "\n",
    "    # Exibir o texto extraído\n",
    "    print(textos)\n",
    "else:\n",
    "    print(\"Erro ao acessar a página:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo 2: Implementando planilha para salvar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planilha criada e salva em: C:\\Users\\MuriloFarias\\Downloads\\Extracao_PGR.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# URL do site\n",
    "url = 'https://onsafety.com.br/pgr/'\n",
    "\n",
    "# Headers para imitar uma requisição de navegador\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Realizar a requisição HTTP com headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar se a requisição foi bem-sucedida\n",
    "if response.status_code == 200:\n",
    "    # Parsear o conteúdo HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Encontrar todos os elementos de título e conteúdo\n",
    "    titulos = soup.find_all(class_='elementor-image-box-title')\n",
    "    conteudos = soup.find_all(class_='elementor-image-box-description')\n",
    "\n",
    "    # Extrair textos e armazenar em listas\n",
    "    lista_titulos = [titulo.get_text(strip=True) for titulo in titulos]\n",
    "    lista_conteudos = [conteudo.get_text(strip=True) for conteudo in conteudos]\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame({'TITULO': lista_titulos, 'CONTEUDO': lista_conteudos})\n",
    "\n",
    "    # Caminho para a pasta de Downloads\n",
    "    caminho_planilha = os.path.join(os.path.expanduser('~'), 'Downloads', 'Extracao_PGR.xlsx')\n",
    "    df.to_excel(caminho_planilha, index=False)\n",
    "\n",
    "    print(\"Planilha criada e salva em:\", caminho_planilha)\n",
    "else:\n",
    "    print(\"Erro ao acessar a página:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo 3: Modificando para ler um arquivo, identifcar os sites depois salvar em outro arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação Geral\n",
    "Importações: Importa as bibliotecas necessárias para fazer requisições HTTP (requests), parsear HTML (BeautifulSoup), manipular dados (pandas), e lidar com caminhos de arquivos (os).\n",
    "\n",
    "Função extrair_dados_site(url):\n",
    "\n",
    "Requisição HTTP: Faz uma requisição ao site especificado usando um cabeçalho de user-agent para evitar bloqueios por bots.\n",
    "Parsing HTML: Utiliza BeautifulSoup para fazer o parsing do conteúdo HTML retornado.\n",
    "Extração de Dados: Encontra todos os elementos HTML com as classes específicas para títulos e conteúdos, extrai o texto desses elementos e retorna duas listas contendo os títulos e os conteúdos.\n",
    "Carregar Dados da Planilha:\n",
    "\n",
    "Caminho da Planilha: Define o caminho da planilha de entrada no diretório Downloads.\n",
    "Leitura da Planilha: Lê a planilha na aba 'Sites' para obter a lista de URLs dos sites.\n",
    "Criar e Salvar a Nova Planilha:\n",
    "\n",
    "Definição do Caminho de Saída: Define o caminho da nova planilha onde os resultados serão salvos.\n",
    "Iteração sobre os Sites: Para cada URL na coluna 'Sites' da planilha:\n",
    "Extrai títulos e conteúdos usando a função extrair_dados_site.\n",
    "Cria um DataFrame com os dados extraídos.\n",
    "Define o nome da aba baseado na URL do site.\n",
    "Escreve o DataFrame em uma nova aba da planilha de saída.\n",
    "Finalização: Após iterar sobre todos os sites, salva a planilha de saída e imprime uma mensagem de confirmação.\n",
    "\n",
    "Este script automatiza a tarefa de extrair e organizar dados de diversos sites em uma única planilha, facilitando a análise e a comparação dos dados extraídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planilha com os resultados criada e salva em: C:\\Users\\MuriloFarias\\Downloads\\Resultados_Sites.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Função para extrair dados de um site\n",
    "def extrair_dados_site(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)  # Faz a requisição HTTP para o site com cabeçalho de user-agent\n",
    "    if response.status_code == 200:  # Verifica se a requisição foi bem-sucedida\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')  # Faz o parsing do HTML do site\n",
    "        titulos = soup.find_all(class_='elementor-image-box-title')  # Encontra todos os elementos com a classe 'elementor-image-box-title'\n",
    "        conteudos = soup.find_all(class_='elementor-image-box-description')  # Encontra todos os elementos com a classe 'elementor-image-box-description'\n",
    "        lista_titulos = [titulo.get_text(strip=True) for titulo in titulos]  # Extrai o texto de cada título e remove espaços em branco extras\n",
    "        lista_conteudos = [conteudo.get_text(strip=True) for conteudo in conteudos]  # Extrai o texto de cada conteúdo e remove espaços em branco extras\n",
    "        return lista_titulos, lista_conteudos  # Retorna listas de títulos e conteúdos\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a página {url}: {response.status_code}\")  # Imprime um erro se a requisição não foi bem-sucedida\n",
    "        return [], []  # Retorna listas vazias em caso de erro\n",
    "\n",
    "# Carregar dados da planilha\n",
    "caminho_planilha = os.path.join(os.path.expanduser('~'), 'Downloads', 'Buscar_Dados.xlsx')  # Define o caminho da planilha de entrada\n",
    "df_sites = pd.read_excel(caminho_planilha, sheet_name='Sites')  # Lê a planilha de entrada na aba 'Sites'\n",
    "\n",
    "# Criar um novo arquivo Excel para salvar os resultados\n",
    "caminho_resultados = os.path.join(os.path.expanduser('~'), 'Downloads', 'Resultados_Sites.xlsx')  # Define o caminho da planilha de saída\n",
    "with pd.ExcelWriter(caminho_resultados, engine='openpyxl') as writer:  # Abre um objeto ExcelWriter para escrever a nova planilha\n",
    "    for site in df_sites['Sites'].dropna():  # Itera sobre cada site na coluna 'Sites', ignorando valores nulos\n",
    "        titulos, conteudos = extrair_dados_site(site)  # Extrai os títulos e conteúdos do site atual\n",
    "        if titulos and conteudos:  # Verifica se foram encontrados títulos e conteúdos\n",
    "            df_site = pd.DataFrame({'TITULO': titulos, 'CONTEUDO': conteudos})  # Cria um DataFrame com os títulos e conteúdos\n",
    "            nome_aba = site.split('/')[-2] if site.split('/')[-1] == '' else site.split('/')[-1]  # Define o nome da aba com base na URL\n",
    "            df_site.to_excel(writer, sheet_name=nome_aba, index=False)  # Escreve o DataFrame na aba correspondente\n",
    "\n",
    "print(\"Planilha com os resultados criada e salva em:\", caminho_resultados)  # Imprime uma mensagem confirmando a criação da planilha\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
