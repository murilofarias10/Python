{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JFaD-q8pPQ3F"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PCA on California Housing Dataset (Google Colab Version)\n",
        "# ============================================================\n",
        "\n",
        "# Step 1: Upload the dataset to Google Colab\n",
        "#from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "BVPegbejPWeS",
        "outputId": "bf3312b0-8b2b-446f-bb6f-1c0dd2315a53"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Upload CSV file from your computer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n\u001b[0;32m      5\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(uploaded\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# get uploaded filename\u001b[39;00m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(io\u001b[38;5;241m.\u001b[39mBytesIO(uploaded[filename]))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------\n",
        "# Upload CSV file from your computer\n",
        "# ------------------------------------------\n",
        "#uploaded = files.upload()\n",
        "#filename = list(uploaded.keys())[0]  # get uploaded filename\n",
        "#data = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "#print(\"File uploaded successfully!\")\n",
        "#print(f\"Dataset shape: {data.shape}\")\n",
        "#print(\"Columns:\", list(data.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnfRm86oPazu",
        "outputId": "6b699d42-c4fe-4b98-8ba2-3fe5c35b22c1"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Step 2: Handle missing values\n",
        "# ----------------------------------------------\n",
        "print(\"\\nChecking for missing values...\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Fill missing numeric values with the mean of their column\n",
        "data = data.fillna(data.mean(numeric_only=True))\n",
        "\n",
        "print(\"\\nMissing values handled (filled with column means).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "pRmorLbKP6lt",
        "outputId": "8b7a829b-8d51-447a-afa6-257d8c288d38"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Step 3: Handle categorical data (ocean_proximity)\n",
        "# ----------------------------------------------\n",
        "# Convert the 'ocean_proximity' column (text) into numeric form\n",
        "# using one-hot encoding (it creates separate columns for each category)\n",
        "data_encoded = pd.get_dummies(data, columns=['ocean_proximity'], drop_first=True)\n",
        "\n",
        "# Use the following 2 lines if you want to see 0 and 1 in the DataFrame instead of False/True. Convert only dummy columns to int\n",
        "dummy_cols = data_encoded.columns[data_encoded.dtypes == 'bool']\n",
        "data_encoded[dummy_cols] = data_encoded[dummy_cols].astype(int)\n",
        "\n",
        "# Check dataframe now\n",
        "data_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "l826FJA-P8g8",
        "outputId": "4a80bc68-112d-4745-a5c2-6da705a79c64"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Step 4: Standardize numeric columns (Standardization)\n",
        "# ----------------------------------------------\n",
        "# Subtract mean and divide by standard deviation\n",
        "data_scaled_df = (data_encoded - data_encoded.mean()) / data_encoded.std()\n",
        "\n",
        "# Check the result\n",
        "data_scaled_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRTET1ZuQJ9X",
        "outputId": "1938b9bb-f411-46d8-9c64-b25471c8a6ab"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# Step 5: Split into train and test subsets\n",
        "# ----------------------------------------------\n",
        "train_data, test_data = train_test_split(data_scaled_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining data shape: {train_data.shape}\")\n",
        "print(f\"Testing data shape: {test_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2uFpzcHQPgS"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# Step 6: Perform PCA\n",
        "# ------------------------------------------\n",
        "pca = PCA()\n",
        "pca.fit(train_data)\n",
        "\n",
        "# Transform both train and test sets\n",
        "train_pca = pca.transform(train_data)\n",
        "test_pca = pca.transform(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0kadxQFQgKc",
        "outputId": "34e1c8f7-f36a-4070-ce6e-7b7f8aa79660"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# Step 7: Show explained variance\n",
        "# ------------------------------------------\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cum_explained_variance = np.cumsum(explained_variance)\n",
        "\n",
        "print(\"\\nExplained Variance Ratio (first 10 PCs):\")\n",
        "for i, var in enumerate(explained_variance[:13]):\n",
        "    print(f\"PC{i+1}: {var:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "RbzO7gtKRJyc",
        "outputId": "6c8f0cca-0bbc-4ab2-f9d9-7f33ee3cdeeb"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# Step 8: Scree Plot\n",
        "# ------------------------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance)+1), cum_explained_variance, 'bo-', linewidth=2)\n",
        "plt.title('Scree Plot (Cumulative Explained Variance)')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXsuqk5HRLlq",
        "outputId": "74397fe7-98f4-4563-b777-e155bf65915c"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# Step 9: Choose top components (optional)\n",
        "# ------------------------------------------\n",
        "# You can select components that explain, e.g., 95% variance:\n",
        "n_components_95 = np.argmax(cum_explained_variance >= 0.95) + 1\n",
        "print(f\"\\n Number of components explaining 95% variance: {n_components_95}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "tpRStareRZ-Z",
        "outputId": "f1df12bb-b024-445a-d1a6-8a5aa189018b"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------\n",
        "# Step 10: Save PCA Loadings with Attribute Names (sorted)\n",
        "\n",
        "# Loadings = eigenvectors * sqrt(eigenvalues)\n",
        "# They show how strongly each feature influences each principal component.\n",
        "# ------------------------------------------\n",
        "# Create loadings DataFrame\n",
        "loadings_df = pd.DataFrame(\n",
        "    pca.components_[:n_components_95].T,\n",
        "    columns=[f'PC{i+1}' for i in range(n_components_95)],\n",
        "    index=train_data.columns\n",
        ")\n",
        "\n",
        "# Reset index to show attribute names as a column\n",
        "loadings_df.reset_index(inplace=True)\n",
        "loadings_df.rename(columns={'index': 'Attribute'}, inplace=True)\n",
        "\n",
        "# Sort by absolute loading strength for the first principal component (PC1) — or change to any PC\n",
        "sorted_loadings = loadings_df.reindex(\n",
        "    loadings_df['PC1'].abs().sort_values(ascending=False).index\n",
        ")\n",
        "\n",
        "# Save both unsorted and sorted loadings\n",
        "loadings_df.to_csv('pca_loadings_unsorted.csv', index=False)\n",
        "sorted_loadings.to_csv('pca_loadings_sorted_PC1.csv', index=False)\n",
        "\n",
        "# Download from Colab\n",
        "from google.colab import files\n",
        "files.download('pca_loadings_sorted_PC1.csv')\n",
        "\n",
        "print(\"\\nPCA loadings sorted by strongest contributors to PC1 and saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "oIQ0PFtXRmHX",
        "outputId": "068db05b-c405-4bd0-9268-4c4a79dfbb0f"
      },
      "outputs": [],
      "source": [
        "# STEP 11: Model Evaluation\n",
        "# Compute variance of each principal component in test data\n",
        "\n",
        "# axis = 0 means, compute variance column-wise i.e., compute variance of each PC across all observations\n",
        "test_pc_variance = np.var(test_pca, axis=0)\n",
        "\n",
        "# Compute fraction of total variance explained\n",
        "test_explained_variance_ratio = test_pc_variance / np.sum(np.var(test_data, axis=0))\n",
        "test_cum_variance = np.cumsum(test_explained_variance_ratio)\n",
        "\n",
        "#  Display results\n",
        "explained_df = pd.DataFrame({\n",
        "    'PC': [f'PC{i+1}' for i in range(len(test_explained_variance_ratio))],\n",
        "    'Explained_Variance_Ratio': test_explained_variance_ratio,\n",
        "    'Cumulative_Explained_Variance': test_cum_variance\n",
        "})\n",
        "\n",
        "print(\"\\n PCA Explained Variance on Test Data:\")\n",
        "print(explained_df.head(10))  # show first 10 PCs\n",
        "\n",
        "# Optional: Scree plot\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(test_cum_variance)+1), test_cum_variance, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot (Cumulative Explained Variance) - Test Data')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r-d9AAgVL-U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
