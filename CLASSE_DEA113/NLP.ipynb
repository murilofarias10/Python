{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ev1Uyoz-gY4e",
        "outputId": "c50ac85d-87f2-4c8e-db36-e8734af2f079"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I love learning Python for data analytics.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Natural language processing helps computers un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Chatbots are used in customer support.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Text data is messy but very powerful.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>NLP is an exciting field of artificial intelli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doc_id                                               text\n",
              "0       1         I love learning Python for data analytics.\n",
              "1       2  Natural language processing helps computers un...\n",
              "2       3             Chatbots are used in customer support.\n",
              "3       4              Text data is messy but very powerful.\n",
              "4       5  NLP is an exciting field of artificial intelli..."
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# In the next line, we'll create a dictionary called data.\n",
        "# A dictionary in Python stores data as key–value pairs.\n",
        "# The first key, \"doc_id\", represents a unique identifier for each document or text entry.\n",
        "# These IDs help us keep track of individual text records.\n",
        "# The second key, \"text\", contains a list of sentences.\n",
        "# These sentences are examples of text data, which is what we typically work with in Natural Language Processing (NLP).\n",
        "# Notice that each sentence is written as a string and placed inside a list.\n",
        "# Each sentence can be thought of as a separate document.\n",
        "# We then convert this dictionary into a pandas DataFrame using pd.DataFrame(data).\n",
        "\n",
        "data = {\n",
        "    \"doc_id\": [1, 2, 3, 4, 5],\n",
        "    \"text\": [\n",
        "        \"I love learning Python for data analytics.\",\n",
        "        \"Natural language processing helps computers understand humans.\",\n",
        "        \"Chatbots are used in customer support.\",\n",
        "        \"Text data is messy but very powerful.\",\n",
        "        \"NLP is an exciting field of artificial intelligence.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\MuriloFarias\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\MuriloFarias\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Next: Tokenization\n",
        "# Tokenization is a foundational step in NLP.\n",
        "# Almost all text analysis tasks—such as word frequency, sentiment analysis, or text classification—start with tokenizing text into sentences or words.\n",
        "# At this point, we are still preparing the data, not analyzing meaning yet.\n",
        "# This step helps transform unstructured text into a form that computers can work with.\n",
        "\n",
        "# In the following step, we are introducing NLTK, which stands for Natural Language Toolkit.\n",
        "# It is a popular Python library used for working with human language data.\n",
        "# We import two specific functions from NLTK:\n",
        "# sent_tokenize() for breaking text into sentences\n",
        "# word_tokenize() for breaking text into individual words\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Next, we run nltk.download(\"punkt\").\n",
        "# This downloads a pretrained tokenizer model that NLTK uses to correctly identify sentence boundaries and words, including punctuation.\n",
        "\n",
        "# We also download punkt_tab which provides supporting lookup tables (metadata) used by the tokenizer. Newer versions of NLTK sometimes require punkt_tab explicitly\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#We then combine all the text from the DataFrame into one single string using join function.\n",
        "\n",
        "text = \" \".join(df[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I love learning Python for data analytics. Natural language processing helps computers understand humans. Chatbots are used in customer support. Text data is messy but very powerful. NLP is an exciting field of artificial intelligence.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# This joins all rows of the text column together, separated by spaces, so we can process the entire corpus at once.\n",
        "# The variable sentences stores the output of sent_tokenize(text).\n",
        "# This function automatically detects where sentences begin and end, even when punctuation is involved.\n",
        "\n",
        "sentences = sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I love learning Python for data analytics.', 'Natural language processing helps computers understand humans.', 'Chatbots are used in customer support.', 'Text data is messy but very powerful.', 'NLP is an exciting field of artificial intelligence.']\n"
          ]
        }
      ],
      "source": [
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrjIQaFRgyce",
        "outputId": "aabf4d73-09ef-4f5e-c9be-c3e042843bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'love', 'learning', 'Python', 'for', 'data', 'analytics', '.', 'Natural', 'language', 'processing', 'helps', 'computers', 'understand', 'humans', '.', 'Chatbots', 'are', 'used', 'in', 'customer', 'support', '.', 'Text', 'data', 'is', 'messy', 'but', 'very', 'powerful']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#The variable words stores the output of word_tokenize(text).\n",
        "#This splits the text into individual tokens, which include words and punctuation marks.\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# When we print sentences, we see a list where each element is a complete sentence.\n",
        "# When we print words[:30], we display only the first 30 tokens.\n",
        "\n",
        "\n",
        "print(words[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next: Removing Stop Words and Cleaning Text\n",
        "\n",
        "# Removing stop words is an important preprocessing step in NLP because it:\n",
        "# Reduces noise in the data\n",
        "# Improves efficiency\n",
        "# Helps models focus on informative words\n",
        "# After this step, the text is ready for further analysis\n",
        "\n",
        "# At this stage, we are cleaning our tokenized text so that it becomes more useful for analysis.\n",
        "# We start by importing stopwords from the nltk.corpus module.\n",
        "# Stop words are very common words in a language that usually do not add much meaning to text analysis.\n",
        "# Examples of stop words include words such as the, is, and, to, in, of.\n",
        "# These words appear very frequently but don’t help us understand what the text is about.\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\MuriloFarias\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This downloads a predefined list of English stop words provided by NLTK.\n",
        "\n",
        "nltk.download(\"stopwords\") #Downloads the stop-words dataset to disk (your computer / Colab environment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words2 = stopwords.words(\"english\") # this is a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#stop_words = stopwords.words(\"english\") # this is a list\n",
        "\n",
        "# We then create a variable called stop_words:\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) # this is a set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv9h9fL_hjfj",
        "outputId": "f7e55a25-9cd6-4159-b7f1-f4e0257f646f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# We convert the list of stop words into a set because sets allow much faster lookups when checking whether a word should be removed.\n",
        "# Now we create a new list called clean_words using a list comprehension.\n",
        "# This single block of code performs multiple cleaning steps at once:\n",
        "# w.lower() converts each word to lowercase, so that words like Python and python are treated the same.\n",
        "# w.isalpha() keeps only alphabetic tokens, removing numbers and punctuation.\n",
        "# w.lower() not in stop_words removes common stop words from the text.\n",
        "# As a result, clean_words contains only:\n",
        "# Lowercase words\n",
        "# Alphabet-only tokens\n",
        "# Words that carry more meaningful information.\n",
        "\n",
        "clean_words = [\n",
        "    w.lower() for w in words\n",
        "    if w.isalpha() and w.lower() not in stop_words\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['love',\n",
              " 'learning',\n",
              " 'python',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'helps',\n",
              " 'computers',\n",
              " 'understand',\n",
              " 'humans',\n",
              " 'chatbots',\n",
              " 'used',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'text',\n",
              " 'data',\n",
              " 'messy',\n",
              " 'powerful',\n",
              " 'nlp',\n",
              " 'exciting',\n",
              " 'field',\n",
              " 'artificial',\n",
              " 'intelligence']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This shows the first 30 cleaned words, which helps us quickly inspect whether the cleaning process worked as expected without printing the entire list.\n",
        "\n",
        "clean_words[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "l867ypbDFipt",
        "outputId": "e3d0b052-501a-4882-92d3-89ddbec833a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['python', 'python', 'data']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'data', 'python'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Python Recap from previous semesters: List is ordered and allow duplicates. Sets are unordered and remove duplicates\n",
        "my_list = [\"python\", \"python\", \"data\"]\n",
        "display(my_list)\n",
        "\n",
        "my_set = {\"python\", \"python\", \"data\"}\n",
        "display(my_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next: Lemmatization using WordNet\n",
        "# Lemmatization converts words into their base or dictionary form, called a lemma.\n",
        "# For example:\n",
        "# running → run\n",
        "# cars → car\n",
        "# better → good (with additional context)\n",
        "\n",
        "# Lemmatization is useful because it:\n",
        "# Reduces word variation\n",
        "# Improves consistency\n",
        "# Helps models treat related words as the same feature\n",
        "\n",
        "# We begin by importing WordNetLemmatizer from nltk.stem.\n",
        "# WordNet is a large lexical database of English that helps NLTK understand word meanings and relationships.\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\MuriloFarias\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\MuriloFarias\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Next, we download the required resources:\n",
        "#wordnet provides the dictionary used for lemmatization\n",
        "#omw-1.4 (Open Multilingual WordNet) supports word mappings and improves coverage\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "#We then create a lemmatizer object. This object will be used to convert words into their base forms.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdF6S_xUiGU7",
        "outputId": "508a6a04-d4b3-4686-b423-42927f01feb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['love',\n",
              " 'learning',\n",
              " 'python',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'help',\n",
              " 'computer',\n",
              " 'understand',\n",
              " 'human',\n",
              " 'chatbots',\n",
              " 'used',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'text',\n",
              " 'data',\n",
              " 'messy',\n",
              " 'powerful',\n",
              " 'nlp',\n",
              " 'exciting',\n",
              " 'field',\n",
              " 'artificial',\n",
              " 'intelligence']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Using a list comprehension, we apply lemmatization to each word in clean_words. At this point:\n",
        "# The words are already lowercase\n",
        "# Stop words and punctuation have been removed\n",
        "# Lemmatization further standardizes the text.\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in clean_words]\n",
        "\n",
        "# Inspect the first 30 lemmatized words and confirm that the transformation worked as expected.\n",
        "\n",
        "lemmatized_words[:30]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next: Word Frequency Analysis using NLTK\n",
        "\n",
        "# In this step, we analyze our cleaned and lemmatized text by looking at word frequencies.\n",
        "# We start by importing FreqDist from nltk.probability.\n",
        "# A frequency distribution simply counts how often each word appears in the text.\n",
        "\n",
        "\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKaG71FXiQ9r",
        "outputId": "63e9e999-14d3-4e3c-c8ba-347f06c5faf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('data', 2),\n",
              " ('love', 1),\n",
              " ('learning', 1),\n",
              " ('python', 1),\n",
              " ('analytics', 1),\n",
              " ('natural', 1),\n",
              " ('language', 1),\n",
              " ('processing', 1),\n",
              " ('help', 1),\n",
              " ('computer', 1)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "#Next, we create a frequency distribution object. This goes through the list of lemmatized words and counts each unique word.\n",
        "\n",
        "freq_dist = FreqDist(lemmatized_words)\n",
        "\n",
        "# The following line returns the top 10 most frequent words along with their counts as (word, frequency).\n",
        "\n",
        "freq_dist.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-Bg-qv15iU6j",
        "outputId": "950d1302-ec26-4d8e-c331-9ce5d13c37a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"freq_df\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"help\",\n          \"text\",\n          \"data\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "freq_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-86e52092-cad1-46a8-94e5-d4d8892c87fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>learning</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>python</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>analytics</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86e52092-cad1-46a8-94e5-d4d8892c87fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86e52092-cad1-46a8-94e5-d4d8892c87fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86e52092-cad1-46a8-94e5-d4d8892c87fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        word  count\n",
              "0       data      2\n",
              "1       love      1\n",
              "2   learning      1\n",
              "3     python      1\n",
              "4  analytics      1"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#I will not use this part below because I dont wanna save the file\n",
        "\n",
        "# Next: Converting Word Frequencies into a DataFrame and Saving to a File\n",
        "\n",
        "# freq_dist.most_common() returns a list of tuples in the form (word, frequency)\n",
        "# We convert this list into a DataFrame with two columns:\n",
        "# word → the text token\n",
        "# count → how many times the word appears\n",
        "# This transformation is important because most data analysis tools work with tables, not raw Python objects.\n",
        "# Next, we save the DataFrame to a CSV file\n",
        "\n",
        "#freq_df = pd.DataFrame(freq_dist.most_common(), columns=[\"word\", \"count\"])\n",
        "#freq_df.to_csv(\"word_frequencies.csv\", index=False)\n",
        "\n",
        "#freq_df.head() # shows the first five rows of the DataFrame\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
