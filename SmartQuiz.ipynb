{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ql6EEhphVDtEHzE5Q52VZY779X4rmg8R",
      "authorship_tag": "ABX9TyMc57qJn38Ae8g93zj24EwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murilofarias10/Python/blob/main/SmartQuiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SmartQuiz with AI\n",
        "By Murilo Farias"
      ],
      "metadata": {
        "id": "FepAIY6qbeot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "This application empowers users to upload text (.txt) files and transform the content into interactive fill-in-the-blank quiz questions.\n",
        "Designed to merge technology and education, the tool dynamically generates questions based on the input text.\n",
        "It is ideal for educators, students, and self-learners.\n",
        "Using the AI-driven text processing and a user-friendly interface to ensure the explanation for quiz content with a good experience."
      ],
      "metadata": {
        "id": "qhR3lv-3bhBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step by Step\n",
        "Text Analysis: Analyzes uploaded content and selects key words for blank spaces.\n",
        "Dynamic Quiz Creation: Generates multiple-choice questions with randomized answer options.\n",
        "Interactive Feedback: Presents collapsible answers so users can learn immediately after attempting the question.\n",
        "AI Context Integration: Uses an AI agent to provide additional context or explanations for quiz content.\n",
        "\n",
        "1.   Hugging Face Agent Setup\n",
        "2.   Validate TXT File\n",
        "3.   Generate Quiz Questions\n",
        "4.   Generate IA Response\n",
        "5.   Process File and Generate Output\n",
        "6.   Create Gradio Interface\n",
        "7.   Launching the Application\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EylApU0bjMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q smolagents==1.4.1 gradio==5.11.0 litellm==1.57.4\n",
        "from smolagents import MultiStepAgent, HfApiModel\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "import random\n",
        "import getpass"
      ],
      "metadata": {
        "id": "5j7tTM5Hbggb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a62c1e-abbd-4691-83e8-85ce117d0017"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1 Hugging Face Agent Setup ---\n",
        "# Authenticate to Hugging Face\n",
        "# Load the pre-trained AI model\n",
        "# Integrate the AI agent into the application workflow\n",
        "def create_agent(system_prompt=\"You are an intelligent agent that generates context for quiz questions.\"):\n",
        "    \"\"\"\n",
        "    Creates and returns a Hugging Face-based AI agent.\n",
        "    \"\"\"\n",
        "    # Login to Hugging Face\n",
        "    token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "    login(token)\n",
        "\n",
        "    MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    model = HfApiModel(model_id=MODEL_ID)\n",
        "\n",
        "    # Configure the agent with placeholders\n",
        "    return MultiStepAgent(\n",
        "        tools=[],\n",
        "        verbosity_level=0,\n",
        "        model=model,\n",
        "        system_prompt=system_prompt + \"\\n{{managed_agents_descriptions}}\\n{{authorized_imports}}\"\n",
        "    )\n",
        "\n",
        "agent = create_agent()\n",
        "\n",
        "\n",
        "\n",
        "# --- 2 Validate TXT File ---\n",
        "# It checks the file extension (only .txt is allowed).\n",
        "# It verifies that the file is not empty\n",
        "# Content is extracted and prepared for analysis.\n",
        "def read_txt_file(file):\n",
        "    \"\"\"\n",
        "    Reads the uploaded TXT file and ensures it's valid.\n",
        "    Returns the content or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not file or not file.name.lower().endswith('.txt'):\n",
        "            return \"Error: Please upload a valid TXT file.\"\n",
        "        with open(file.name, 'r', encoding='utf-8') as f:\n",
        "            content = f.read().strip()\n",
        "        return content if content else \"Error: Empty file.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "# --- 3 Generate Quiz Questions ---\n",
        "# Sentence Selection: Sentences are filtered to exclude short or irrelevant ones\n",
        "# Blank Creation: One word in the selected sentence is replaced with \"_____\".\n",
        "# Option Generation: The correct word and two similar but incorrect options are generated.\n",
        "# Shuffling: Options are shuffled to randomize their order.\n",
        "# Output: The formatted question, multiple-choice options, and collapsible correct answer are prepared for display.\n",
        "\n",
        "def create_quiz_questions(content):\n",
        "    \"\"\"\n",
        "    Processes the content and generates a single 'fill-in-the-blank' question.\n",
        "    Includes multiple-choice options and identifies the correct answer.\n",
        "    \"\"\"\n",
        "    if not content or isinstance(content, str) and content.startswith(\"Error\"):\n",
        "        return \"Could not generate question. Please check your file.\"\n",
        "\n",
        "    sentences = [s.strip() for s in content.split('\\n') if len(s.strip()) > 20 and ':' not in s]\n",
        "\n",
        "    if not sentences:\n",
        "        return \"Not enough content to generate a question.\"\n",
        "\n",
        "    all_words = set(word.lower() for sentence in sentences for word in sentence.split()\n",
        "                    if len(word) > 3 and not word.isupper())\n",
        "\n",
        "    if len(all_words) == 0:\n",
        "        return \"Could not generate words for options.\"\n",
        "\n",
        "    sentence = random.choice(sentences)\n",
        "\n",
        "    words = [word for word in sentence.split()\n",
        "             if len(word) > 3 and not word.isupper()\n",
        "             and word.lower() not in ['they', 'their', 'them', 'like', 'with', 'from']]\n",
        "\n",
        "    if not words:\n",
        "        return \"Could not generate a valid question from the content.\"\n",
        "\n",
        "    chosen_word = random.choice(words)\n",
        "    blank_sentence = sentence.replace(chosen_word, \"_____\", 1)\n",
        "\n",
        "    similar_words = [w for w in all_words\n",
        "                     if len(w) >= len(chosen_word) - 2 and len(w) <= len(chosen_word) + 2 and w != chosen_word.lower()]\n",
        "\n",
        "    if len(similar_words) < 2:\n",
        "        return \"Could not generate enough options.\"\n",
        "\n",
        "    wrong_options = random.sample(similar_words, 2)\n",
        "    options = [chosen_word] + wrong_options\n",
        "    random.shuffle(options)\n",
        "\n",
        "    complete_sentence = sentence.replace(chosen_word, chosen_word.capitalize(), 1)\n",
        "    return {\n",
        "        \"question\": blank_sentence,\n",
        "        \"options\": [f\"Option {chr(97 + i)}: {opt}\" for i, opt in enumerate(options)],\n",
        "        \"answer\": chosen_word,\n",
        "        \"correct_option\": f\"Option {chr(97 + options.index(chosen_word))}\",\n",
        "        \"complete_sentence\": complete_sentence\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# --- 4 Generate IA Response ---\n",
        "#Query AI Agent: Sends the complete sentence to the AI agent for additional context or explanation.\n",
        "#AI Response: Receives and displays the AI's response to the complete sentence.\n",
        "\n",
        "def generate_ia_response(complete_sentence):\n",
        "    \"\"\"\n",
        "    Queries the AI agent with the complete sentence and returns the IA's response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ia_response = agent.run(f\"Provide detailed information about: {complete_sentence}\")\n",
        "        return ia_response\n",
        "    except Exception as e:\n",
        "        return f\"Error processing IA response: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "# --- 5 Process File and Generate Output ---\n",
        "# File Processing: Reads the uploaded file and validates its content\n",
        "# Question Creation: Calls the function to generate a quiz question from the content.\n",
        "# Output Formatting: Prepares a clean and interactive Markdown format, including the question, multiple-choice options, and collapsible answers.\n",
        "# Hidden Storage: Stores the complete sentence internally for later use by the AI agent.\n",
        "def process_file(file):\n",
        "    \"\"\"\n",
        "    Reads the file, generates a question, and formats the output.\n",
        "    \"\"\"\n",
        "    content = read_txt_file(file)\n",
        "    if isinstance(content, str) and content.startswith(\"Error\"):\n",
        "        return content\n",
        "\n",
        "    question = create_quiz_questions(content)\n",
        "    if isinstance(question, str):\n",
        "        return question\n",
        "\n",
        "    # Updated formatting with better layout\n",
        "    output = \"### Fill in the blank:\\n\\n\"\n",
        "    output += f\"**{question['question']}**\\n\\n\"\n",
        "\n",
        "    output += \"**Choose the correct answer:**\\n\\n\"\n",
        "    for option in question['options']:\n",
        "        output += f\"* {option}\\n\"\n",
        "\n",
        "    output += f\"\\n<details><summary>👉 Show Answer</summary>\\n\\n\"\n",
        "    output += f\"**Correct answer: {question['answer']} ({question['correct_option']})**\\n\\n\"\n",
        "    output += f\"**Complete sentence:** {question['complete_sentence']}\\n\"\n",
        "    output += \"</details>\\n\"\n",
        "\n",
        "    # Return the complete sentence in a hidden format for the AI to use\n",
        "    return output, question['complete_sentence']\n",
        "\n",
        "def clean_outputs():\n",
        "    \"\"\"\n",
        "    Resets outputs to their initial state while preserving the file input.\n",
        "    \"\"\"\n",
        "    return gr.skip(), \"\", \"\"  # Skip file input reset, clear question output and AI response\n",
        "\n",
        "\n",
        "\n",
        "# --- 6 Create Gradio Interface ---\n",
        "#User Interface Design: Builds an interactive web interface using Gradio\n",
        "#Question and AI Response Display: Includes sections for displaying quiz questions and AI-provided context.\n",
        "# Button: Generate Question: Processes the file and generates a quiz question\n",
        "# Button: Get AI Response: Queries the AI agent for additional context.\n",
        "# Button: Clean: Resets the interface while preserving the file input.\n",
        "def create_interface():\n",
        "    \"\"\"\n",
        "    Builds the Gradio interface with custom styling for the quiz.\n",
        "    \"\"\"\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# 📚 SmartQuiz with AI by: Murilo Farias\")\n",
        "\n",
        "        with gr.Row():\n",
        "            file_input = gr.File(label=\"Upload TXT file\", file_types=[\".txt\"])\n",
        "            submit_btn = gr.Button(\"Generate Question\")\n",
        "            clean_btn = gr.Button(\"Clean\")  # New Clean button\n",
        "\n",
        "        output = gr.Markdown()\n",
        "        hidden_sentence = gr.Markdown(visible=False)  # Hidden storage for the complete sentence\n",
        "        ia_response = gr.Markdown(label=\"Generate AI Context\")  # Visible component for AI response\n",
        "\n",
        "        # Button click actions\n",
        "        submit_btn.click(fn=process_file, inputs=[file_input], outputs=[output, hidden_sentence])\n",
        "        ia_btn = gr.Button(\"Get AI Response\")\n",
        "        ia_btn.click(fn=generate_ia_response, inputs=[hidden_sentence], outputs=[ia_response])\n",
        "\n",
        "        # Clean button action - using gr.skip() to preserve file input\n",
        "        clean_btn.click(\n",
        "            fn=clean_outputs,\n",
        "            inputs=[],\n",
        "            outputs=[file_input, output, ia_response]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "\n",
        "# --- 7 Launching the Application ---\n",
        "#App Initialization: Creates and configures the Gradio interface\n",
        "#Launching: Runs the Gradio interface and starts the application\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "TOp4qq5Xb6mv",
        "outputId": "9910effe-7186-44e8-88e5-450e3bfb7b81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://adcb493b7c40f363a0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://adcb493b7c40f363a0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}