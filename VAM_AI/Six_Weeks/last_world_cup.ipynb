{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "    from together import Together\n",
    "\n",
    "except ImportError:\n",
    "    !pip install -q gradio\n",
    "    !pip install -q together\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from together import Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"content\": \"Contract_V1.pdf\",\n",
    "            \"category\": \"pdf\",\n",
    "            \"source\": \"original_contract\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Contract_V2.pdf\",\n",
    "            \"category\": \"pdf\",\n",
    "            \"source\": \"revised_contract\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Technical_Report_V1.pdf\",\n",
    "            \"category\": \"pdf\",\n",
    "            \"source\": \"initial_technical_report\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Technical_Report_V2.pdf\",\n",
    "            \"category\": \"pdf\",\n",
    "            \"source\": \"updated_technical_report\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Blueprints_Scan.pdf\",\n",
    "            \"category\": \"image_pdf\",\n",
    "            \"source\": \"scanned_blueprint\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Blueprints_Vector.pdf\",\n",
    "            \"category\": \"vector_pdf\",\n",
    "            \"source\": \"CAD_generated_blueprint\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Electrical_Drawing_V1.pdf\",\n",
    "            \"category\": \"vector_pdf\",\n",
    "            \"source\": \"initial_electrical_drawing\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Electrical_Drawing_V2.pdf\",\n",
    "            \"category\": \"vector_pdf\",\n",
    "            \"source\": \"updated_electrical_drawing\",\n",
    "        },\n",
    "    ],\n",
    "    \"expected_output\": [\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What changed in the payment terms of the revised contract?\",\n",
    "            \"options\": [\n",
    "                \"Late payments now incur a 5% penalty instead of 2%.\",\n",
    "                \"Late payments are now waived.\",\n",
    "                \"A new tax was introduced.\",\n",
    "                \"The contract now requires prepayment.\"\n",
    "            ],\n",
    "            \"correct\": 0,\n",
    "            \"justification\": \"The revised contract increased the penalty for late payments from 2% to 5%.\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What was updated in the technical report's efficiency analysis?\",\n",
    "            \"options\": [\n",
    "                \"Efficiency decreased to 80%.\",\n",
    "                \"A new efficiency metric was added.\",\n",
    "                \"Efficiency increased from 85% to 90%.\",\n",
    "                \"The section was removed.\"\n",
    "            ],\n",
    "            \"correct\": 2,\n",
    "            \"justification\": \"The updated report reflects an increase in efficiency from 85% to 90%.\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What structural change was detected in the blueprint?\",\n",
    "            \"options\": [\n",
    "                \"The number of support beams increased from 4 to 5.\",\n",
    "                \"Support beams were removed.\",\n",
    "                \"The building size was reduced.\",\n",
    "                \"A new floor was added.\"\n",
    "            ],\n",
    "            \"correct\": 0,\n",
    "            \"justification\": \"The revised blueprint shows an increase in support beams from 4 to 5.\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What was the main change in the CAD blueprint?\",\n",
    "            \"options\": [\n",
    "                \"The main column rotated from 90 to 85 degrees.\",\n",
    "                \"The blueprint size was doubled.\",\n",
    "                \"New rooms were added.\",\n",
    "                \"The entire building was relocated.\"\n",
    "            ],\n",
    "            \"correct\": 0,\n",
    "            \"justification\": \"The revised blueprint shows that the main column structure was rotated from 90 to 85 degrees.\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What change was detected in the electrical drawing?\",\n",
    "            \"options\": [\n",
    "                \"Transformer rating increased from 500 kVA to 600 kVA.\",\n",
    "                \"Transformer size was reduced.\",\n",
    "                \"A new transformer was removed.\",\n",
    "                \"Transformer efficiency decreased.\"\n",
    "            ],\n",
    "            \"correct\": 0,\n",
    "            \"justification\": \"The revised electrical drawing shows an increase in transformer rating from 500 kVA to 600 kVA.\"\n",
    "        }\n",
    "    ]\n",
    "],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "from together import Together\n",
    "\n",
    "\n",
    "# TODO 1: Replace with your Together API key (https://www.together.ai/)\n",
    "your_api_key = \"9806a2601560024637df1e4acd804862faa67e08637db6598d920b64eebba43e\"\n",
    "client = Together(api_key=your_api_key)\n",
    "\n",
    "\n",
    "def prompt_llm(prompt):\n",
    "    # TODO 2: You can experiment with different models here (see here https://api.together.ai/models)\n",
    "    model = \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_point(index):\n",
    "    input_data = dataset[\"input\"][index]\n",
    "    quiz_data = dataset[\"expected_output\"][index]\n",
    "\n",
    "    # Update header with current index + 1\n",
    "    header_text = f\"## Example {index + 1}\"\n",
    "\n",
    "    # Format the input display with markdown and symbols, now with a box\n",
    "    input_text = \"\"\"\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 8px; padding: 15px; background-color: #ddfff5;\">\n",
    "\n",
    "### üìñ Topic: {content}\n",
    "\n",
    "üè∑Ô∏è **Category**: {category}\n",
    "\n",
    "üîó **Source**: {source}\n",
    "\n",
    "</div>\n",
    "\"\"\".format(\n",
    "        **input_data\n",
    "    )\n",
    "\n",
    "    # Format the output in markdown with styled box\n",
    "    output_text = \"\"\"\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 8px; padding: 15px; background-color: #ddfff5;\">\n",
    "\n",
    "### üìù Quiz Questions\n",
    "\n",
    "\"\"\"\n",
    "    for i, quiz in enumerate(quiz_data, 1):\n",
    "        output_text += f\"#### Q{i}: {quiz['question']}\\n\\n\"\n",
    "        for j, option in enumerate(quiz[\"options\"]):\n",
    "            output_text += f\"{chr(97 + j)}) {option}\\n\\n\"\n",
    "        output_text += f\"\\n‚úÖ **Correct Answer**: {chr(96 + quiz['correct'] + 1)}\\n\"\n",
    "        if \"justification\" in quiz:\n",
    "            output_text += f\"\\nüí° **Explanation**: {quiz['justification']}\\n\"\n",
    "        output_text += \"\\n---\\n\\n\"\n",
    "\n",
    "    output_text += \"</div>\"\n",
    "\n",
    "    return header_text, input_text, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define custom CSS for full dark mode with white borders\n",
    "custom_css = \"\"\"\n",
    "/* Global dark mode */\n",
    "body, .gradio-container {\n",
    "    background-color: #1e1e1e !important;\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    "/* Ensure all blocks, rows, and columns have dark backgrounds */\n",
    ".gr-block, .gr-column, .gr-slider, .gr-button, .gr-row, .gr-box {\n",
    "    background-color: #262626 !important;\n",
    "    color: white !important;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    border: 2px solid white !important;\n",
    "}\n",
    "\n",
    "/* Fix for Markdown elements */\n",
    ".gr-markdown, .gr-markdown h1, .gr-markdown h2, .gr-markdown h3, \n",
    ".gr-markdown h4, .gr-markdown p, .gr-markdown strong, \n",
    ".gr-markdown em, .gr-markdown a, .gr-markdown ul, \n",
    ".gr-markdown li, .gr-markdown pre, .gr-markdown code {\n",
    "    color: white !important;\n",
    "    background-color: transparent !important;\n",
    "    opacity: 1 !important;  \n",
    "}\n",
    "\n",
    "/* Fix for input elements (slider, textbox, textarea) */\n",
    ".gr-slider, .gr-textbox, .gr-textarea, input, textarea {\n",
    "    background-color: #333 !important;\n",
    "    color: white !important;\n",
    "    border: 1px solid #555 !important;\n",
    "}\n",
    "\n",
    "/* Fix for Gradio UI components */\n",
    ".gradio-container * {\n",
    "    background-color: #1e1e1e !important;\n",
    "    color: white !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # üìö    PDFusion\n",
    "    AI-Powered PDF Comparison.\n",
    "    \"\"\"\n",
    "    )\n",
    "    header = gr.Markdown(\"## Example 1\")\n",
    "\n",
    "    with gr.Row():\n",
    "        index_slider = gr.Slider(\n",
    "            minimum=0,\n",
    "            maximum=len(dataset[\"input\"]) - 1,\n",
    "            step=1,\n",
    "            value=0,\n",
    "            label=\"Data Point Index\",\n",
    "            container=False,\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        # Input section with prompt\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Input\")\n",
    "            with gr.Row():\n",
    "                input_text = gr.Markdown(\n",
    "                    label=\"Input\",\n",
    "                    value=show_data_point(0)[1],\n",
    "                )\n",
    "        with gr.Column():\n",
    "            run_prompt_btn = gr.Button(\"Run Prompt\")\n",
    "\n",
    "            prompt_llm_text = gr.TextArea(\n",
    "                label=\"Prompt Template\",\n",
    "                value=\"\"\"You are an expert quiz generator, skilled at creating engaging and educational multiple-choice questions.\n",
    "\n",
    "# Task Description\n",
    "Generate 3 multiple-choice questions about the given topic with 4\n",
    "options each. Include explanations for the correct answers.\n",
    "\n",
    "# Output Format\n",
    "- Q1: Question 1\n",
    "- A1: Option 1\n",
    "- A2: Option 2\n",
    "- A3: Option 3\n",
    "- A4: Option 4\n",
    "- Correct Answer: 1\n",
    "- Explanation: Explanation for the correct answer\n",
    "\n",
    "# Input Content\n",
    "{content}\n",
    "\n",
    "# Requirements\n",
    "1. Each question must have exactly 4 options\n",
    "2. Include clear explanations for correct answers\n",
    "3. Ensure questions test understanding, not just memorization\n",
    "4. Use clear, concise language\n",
    "5. Make sure all options are plausible\"\"\",\n",
    "                lines=10,\n",
    "                interactive=True,\n",
    "                container=True,\n",
    "                show_label=True,\n",
    "                elem_classes=\"custom-textarea\",\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Expected Output\")\n",
    "            output_text = gr.Markdown(\n",
    "                label=\"Expected Output\",\n",
    "                value=show_data_point(0)[2],\n",
    "            )\n",
    "\n",
    "        # Output section with LLM output and expected output\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## LLM Output\")\n",
    "            llm_output = gr.TextArea(\n",
    "                label=\"LLM Output\",\n",
    "                value=\"LLM output will appear here...\",\n",
    "                lines=10,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                approve_btn = gr.Button(\"Approve and Save\")\n",
    "                save_status = gr.Markdown(\"\")\n",
    "\n",
    "    # Function to save approved output\n",
    "    def save_approved_output(index, prompt_template, llm_output):\n",
    "        input_data = dataset[\"input\"][index]\n",
    "        output_data = {\n",
    "            \"input\": input_data,\n",
    "            \"prompt\": prompt_template,\n",
    "            \"llm_output\": llm_output,\n",
    "        }\n",
    "\n",
    "        # Save to JSON file with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"approved_outputs_{timestamp}.json\"\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(output_data, f, indent=4)\n",
    "\n",
    "        return f\"‚úÖ Saved! File: `{filename}`\"\n",
    "\n",
    "    # Connect the approve button\n",
    "    approve_btn.click(\n",
    "        save_approved_output,\n",
    "        inputs=[index_slider, prompt_llm_text, llm_output],\n",
    "        outputs=[save_status],\n",
    "    )\n",
    "\n",
    "    index_slider.change(\n",
    "        show_data_point,\n",
    "        inputs=[index_slider],\n",
    "        outputs=[header, input_text, output_text],\n",
    "    )\n",
    "\n",
    "    def generate_llm_response(index, prompt_template):\n",
    "        input_data = dataset[\"input\"][index]\n",
    "        prompt = prompt_template.format(content=input_data[\"content\"])\n",
    "        response = prompt_llm(prompt)\n",
    "        return response\n",
    "\n",
    "    # Connect the run prompt button\n",
    "    run_prompt_btn.click(\n",
    "        generate_llm_response,\n",
    "        inputs=[index_slider, prompt_llm_text],\n",
    "        outputs=[llm_output],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 2045, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\utils.py\", line 870, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Temp\\ipykernel_26976\\1390630448.py\", line 21, in show_data_point\n",
      "    quiz_data = dataset[\"expected_output\"][index]\n",
      "                ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "from together import Together\n",
    "\n",
    "# ‚úÖ API Setup\n",
    "your_api_key = \"9806a2601560024637df1e4acd804862faa67e08637db6598d920b64eebba43e\"\n",
    "client = Together(api_key=your_api_key)\n",
    "\n",
    "# ‚úÖ Function to Send Prompt to LLM\n",
    "def prompt_llm(prompt):\n",
    "    model = \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ‚úÖ Function to Get Data Point\n",
    "def show_data_point(index):\n",
    "    input_data = dataset[\"input\"][index]\n",
    "    quiz_data = dataset[\"expected_output\"][index]\n",
    "\n",
    "    # Header update\n",
    "    header_text = f\"## Example {index + 1}\"\n",
    "\n",
    "    # Format input display\n",
    "    input_text = \"\"\"\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 8px; padding: 15px; background-color: #ddfff5;\">\n",
    "### üìñ Topic: {content}\n",
    "üè∑Ô∏è **Category**: {category}\n",
    "üîó **Source**: {source}\n",
    "</div>\n",
    "\"\"\".format(\n",
    "        **input_data\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    output_text = \"\"\"\n",
    "<div style=\"border: 2px solid #ddd; border-radius: 8px; padding: 15px; background-color: #ddfff5;\">\n",
    "### üìù Quiz Questions\n",
    "\"\"\"\n",
    "    for i, quiz in enumerate(quiz_data, 1):\n",
    "        output_text += f\"#### Q{i}: {quiz['question']}\\n\\n\"\n",
    "        for j, option in enumerate(quiz[\"options\"]):\n",
    "            output_text += f\"{chr(97 + j)}) {option}\\n\\n\"\n",
    "        output_text += f\"\\n‚úÖ **Correct Answer**: {chr(96 + quiz['correct'] + 1)}\\n\"\n",
    "        if \"justification\" in quiz:\n",
    "            output_text += f\"\\nüí° **Explanation**: {quiz['justification']}\\n\"\n",
    "        output_text += \"\\n---\\n\\n\"\n",
    "    output_text += \"</div>\"\n",
    "\n",
    "    return header_text, input_text, output_text\n",
    "\n",
    "# ‚úÖ Function to Generate LLM Response\n",
    "def generate_llm_response(index, prompt_template):\n",
    "    try:\n",
    "        input_data = dataset[\"input\"][index]\n",
    "        prompt = prompt_template.format(content=input_data[\"content\"])  # Format prompt\n",
    "\n",
    "        print(f\"üîç Sending prompt to LLM:\\n{prompt}\")  # Debug log\n",
    "\n",
    "        response = prompt_llm(prompt)  # Call the LLM\n",
    "\n",
    "        if not response:  # Handle empty response\n",
    "            return \"‚ö†Ô∏è LLM did not return any output. Check API key or prompt format.\"\n",
    "\n",
    "        print(f\"‚úÖ Received LLM response:\\n{response}\")  # Debug log\n",
    "\n",
    "        return response  # Return the generated response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while generating LLM response: {e}\")  # Debugging error\n",
    "        return f\"‚ö†Ô∏è Error: {e}\"\n",
    "\n",
    "# ‚úÖ Create Gradio Interface\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import json\n",
    "from together import Together\n",
    "\n",
    "# ‚úÖ Custom CSS for Dark Mode (Place This at the Top)\n",
    "custom_css = \"\"\"\n",
    "/* Global dark mode */\n",
    "body, .gradio-container {\n",
    "    background-color: #1e1e1e !important;\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    "/* Ensure all blocks, rows, and columns have dark backgrounds */\n",
    ".gr-block, .gr-column, .gr-slider, .gr-button, .gr-row, .gr-box {\n",
    "    background-color: #262626 !important;\n",
    "    color: white !important;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    border: 2px solid white !important;\n",
    "}\n",
    "\n",
    "/* Fix for Markdown elements */\n",
    ".gr-markdown, .gr-markdown h1, .gr-markdown h2, .gr-markdown h3, \n",
    ".gr-markdown h4, .gr-markdown p, .gr-markdown strong, \n",
    ".gr-markdown em, .gr-markdown a, .gr-markdown ul, \n",
    ".gr-markdown li, .gr-markdown pre, .gr-markdown code {\n",
    "    color: white !important;\n",
    "    background-color: transparent !important;\n",
    "    opacity: 1 !important;  \n",
    "}\n",
    "\n",
    "/* Fix for input elements (slider, textbox, textarea) */\n",
    ".gr-slider, .gr-textbox, .gr-textarea, input, textarea {\n",
    "    background-color: #333 !important;\n",
    "    color: white !important;\n",
    "    border: 1px solid #555 !important;\n",
    "}\n",
    "\n",
    "/* Fix for Gradio UI components */\n",
    ".gradio-container * {\n",
    "    background-color: #1e1e1e !important;\n",
    "    color: white !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ‚úÖ Apply custom CSS inside gr.Blocks(...)\n",
    "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # üìö Omniscient Prompt XRay\n",
    "    Explore quiz questions generated for different topics.\n",
    "    \"\"\"\n",
    "    )\n",
    "    header = gr.Markdown(\"## Example 1\")\n",
    "\n",
    "    with gr.Row():\n",
    "        index_slider = gr.Slider(\n",
    "            minimum=0,\n",
    "            maximum=len(dataset[\"input\"]) - 1,\n",
    "            step=1,\n",
    "            value=0,\n",
    "            label=\"Data Point Index\",\n",
    "            container=False,\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Input\")\n",
    "            with gr.Row():\n",
    "                input_text = gr.Markdown(\n",
    "                    label=\"Input\",\n",
    "                    value=show_data_point(0)[1],\n",
    "                )\n",
    "\n",
    "        with gr.Column():\n",
    "            run_prompt_btn = gr.Button(\"Run Prompt\")\n",
    "\n",
    "            prompt_llm_text = gr.TextArea(\n",
    "                label=\"Prompt Template\",\n",
    "                value=\"\"\"You are an expert quiz generator, skilled at creating engaging and educational multiple-choice questions.\n",
    "\n",
    "# Task Description\n",
    "Generate 3 multiple-choice questions about the given topic with 4\n",
    "options each. Include explanations for the correct answers.\n",
    "\n",
    "# Output Format\n",
    "- Q1: Question 1\n",
    "- A1: Option 1\n",
    "- A2: Option 2\n",
    "- A3: Option 3\n",
    "- A4: Option 4\n",
    "- Correct Answer: 1\n",
    "- Explanation: Explanation for the correct answer\n",
    "\n",
    "# Input Content\n",
    "{content}\n",
    "\n",
    "# Requirements\n",
    "1. Each question must have exactly 4 options\n",
    "2. Include clear explanations for correct answers\n",
    "3. Ensure questions test understanding, not just memorization\n",
    "4. Use clear, concise language\n",
    "5. Make sure all options are plausible\"\"\",\n",
    "                lines=10,\n",
    "                interactive=True,\n",
    "                container=True,\n",
    "                show_label=True,\n",
    "                elem_classes=\"custom-textarea\",\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Expected Output\")\n",
    "            output_text = gr.Markdown(\n",
    "                label=\"Expected Output\",\n",
    "                value=show_data_point(0)[2],\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## LLM Output\")\n",
    "            llm_output = gr.TextArea(\n",
    "                label=\"LLM Output\",\n",
    "                value=\"LLM output will appear here...\",\n",
    "                lines=10,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                approve_btn = gr.Button(\"Approve and Save\")\n",
    "                save_status = gr.Markdown(\"\")\n",
    "\n",
    "    # ‚úÖ Connect LLM Generation to Button\n",
    "    run_prompt_btn.click(\n",
    "        generate_llm_response,  # Calls function\n",
    "        inputs=[index_slider, prompt_llm_text],  # Uses index & prompt template\n",
    "        outputs=[llm_output],  # Updates the LLM output field\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Update Interface When Slider Changes\n",
    "    index_slider.change(\n",
    "        show_data_point,\n",
    "        inputs=[index_slider],\n",
    "        outputs=[header, input_text, output_text],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 2045, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\utils.py\", line 870, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MuriloFarias\\AppData\\Local\\Temp\\ipykernel_26976\\3500517219.py\", line 67, in show_data_point\n",
      "    quiz_data = dataset[\"expected_output\"][index]\n",
      "                ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "from together import Together\n",
    "\n",
    "# ‚úÖ Set up Together AI API\n",
    "your_api_key = \"9806a2601560024637df1e4acd804862faa67e08637db6598d920b64eebba43e\"\n",
    "client = Together(api_key=your_api_key)\n",
    "\n",
    "# ‚úÖ Custom CSS for Full Dark Mode\n",
    "custom_css = \"\"\"\n",
    "/* Global dark mode */\n",
    "body, .gradio-container {\n",
    "    background-color: #1e1e1e !important;\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    "/* Ensure all blocks, rows, and columns have dark backgrounds */\n",
    ".gr-block, .gr-column, .gr-slider, .gr-button, .gr-row, .gr-box {\n",
    "    background-color: #262626 !important;\n",
    "    color: white !important;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    border: 2px solid white !important;\n",
    "}\n",
    "\n",
    "/* Fix for Markdown elements */\n",
    ".gr-markdown, .gr-markdown h1, .gr-markdown h2, .gr-markdown h3, \n",
    ".gr-markdown h4, .gr-markdown p, .gr-markdown strong, \n",
    ".gr-markdown em, .gr-markdown a, .gr-markdown ul, \n",
    ".gr-markdown li, .gr-markdown pre, .gr-markdown code {\n",
    "    color: white !important;\n",
    "    background-color: transparent !important;\n",
    "    opacity: 1 !important;  \n",
    "}\n",
    "\n",
    "/* Fix for input elements (slider, textbox, textarea) */\n",
    ".gr-slider, .gr-textbox, .gr-textarea, input, textarea {\n",
    "    background-color: #333 !important;\n",
    "    color: white !important;\n",
    "    border: 1px solid #555 !important;\n",
    "}\n",
    "\n",
    "/* Fix for Gradio UI components */\n",
    ".gradio-container * {\n",
    "    background-color: #1e1e1e !important;\n",
    "    color: white !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ‚úÖ Function to Call LLM API\n",
    "def prompt_llm(prompt):\n",
    "    try:\n",
    "        model = \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return response.choices[0].message.content  # Return the LLM-generated response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API Error: {e}\")  # Debugging\n",
    "        return f\"‚ö†Ô∏è Error: {e}\"\n",
    "\n",
    "# ‚úÖ Function to Display a Data Point\n",
    "def show_data_point(index):\n",
    "    input_data = dataset[\"input\"][index]\n",
    "    quiz_data = dataset[\"expected_output\"][index]\n",
    "\n",
    "    # Header update\n",
    "    header_text = f\"## Example {index + 1}\"\n",
    "\n",
    "    # Format input display\n",
    "    input_text = \"\"\"\n",
    "<div style=\"border: 2px solid white; border-radius: 8px; padding: 15px; background-color: #262626;\">\n",
    "### üìñ Topic: {content}\n",
    "üè∑Ô∏è **Category**: {category}\n",
    "üîó **Source**: {source}\n",
    "</div>\n",
    "\"\"\".format(\n",
    "        **input_data\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    output_text = \"\"\"\n",
    "<div style=\"border: 2px solid white; border-radius: 8px; padding: 15px; background-color: #262626;\">\n",
    "### üìù Quiz Questions\n",
    "\"\"\"\n",
    "    for i, quiz in enumerate(quiz_data, 1):\n",
    "        output_text += f\"#### Q{i}: {quiz['question']}\\n\\n\"\n",
    "        for j, option in enumerate(quiz[\"options\"]):\n",
    "            output_text += f\"{chr(97 + j)}) {option}\\n\\n\"\n",
    "        output_text += f\"\\n‚úÖ **Correct Answer**: {chr(96 + quiz['correct'] + 1)}\\n\"\n",
    "        if \"justification\" in quiz:\n",
    "            output_text += f\"\\nüí° **Explanation**: {quiz['justification']}\\n\"\n",
    "        output_text += \"\\n---\\n\\n\"\n",
    "    output_text += \"</div>\"\n",
    "\n",
    "    return header_text, input_text, output_text\n",
    "\n",
    "# ‚úÖ Function to Generate LLM Response\n",
    "def generate_llm_response(index, prompt_template):\n",
    "    try:\n",
    "        input_data = dataset[\"input\"][index]  # Get the input data for selected index\n",
    "        prompt = prompt_template.format(content=input_data[\"content\"])  # Format prompt\n",
    "\n",
    "        print(f\"üîç Sending prompt to LLM:\\n{prompt}\")  # Debug log\n",
    "\n",
    "        response = prompt_llm(prompt)  # Call the LLM API\n",
    "\n",
    "        if not response or response.strip() == \"\":\n",
    "            print(\"‚ö†Ô∏è LLM returned an empty response.\")\n",
    "            return \"‚ö†Ô∏è LLM did not return any output. Check API key or prompt format.\"\n",
    "\n",
    "        print(f\"‚úÖ Received LLM response:\\n{response}\")  # Debug log\n",
    "\n",
    "        return response  # Return the generated response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while generating LLM response: {e}\")  # Debugging error\n",
    "        return f\"‚ö†Ô∏è Error: {e}\"\n",
    "\n",
    "# ‚úÖ Create Gradio Interface\n",
    "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# üìö Omniscient Prompt XRay\\nExplore quiz questions generated for different topics.\")\n",
    "\n",
    "    header = gr.Markdown(\"## Example 1\")\n",
    "\n",
    "    with gr.Row():\n",
    "        index_slider = gr.Slider(\n",
    "            minimum=0,\n",
    "            maximum=len(dataset[\"input\"]) - 1,\n",
    "            step=1,\n",
    "            value=0,\n",
    "            label=\"Data Point Index\",\n",
    "            container=False,\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Input\")\n",
    "            with gr.Row():\n",
    "                input_text = gr.Markdown(\n",
    "                    label=\"Input\",\n",
    "                    value=show_data_point(0)[1],\n",
    "                )\n",
    "\n",
    "        with gr.Column():\n",
    "            run_prompt_btn = gr.Button(\"Run Prompt\")\n",
    "\n",
    "            prompt_llm_text = gr.TextArea(\n",
    "                label=\"Prompt Template\",\n",
    "                value=\"\"\"You are an expert quiz generator, skilled at creating engaging and educational multiple-choice questions.\n",
    "\n",
    "# Task Description\n",
    "Generate 3 multiple-choice questions about the given topic with 4\n",
    "options each. Include explanations for the correct answers.\n",
    "\n",
    "# Input Content\n",
    "{content}\n",
    "\n",
    "# Requirements\n",
    "1. Each question must have exactly 4 options\n",
    "2. Include clear explanations for correct answers\n",
    "3. Ensure questions test understanding, not just memorization\n",
    "4. Use clear, concise language\n",
    "5. Make sure all options are plausible\"\"\",\n",
    "                lines=10,\n",
    "                interactive=True,\n",
    "                container=True,\n",
    "                show_label=True,\n",
    "                elem_classes=\"custom-textarea\",\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Expected Output\")\n",
    "            output_text = gr.Markdown(\n",
    "                label=\"Expected Output\",\n",
    "                value=show_data_point(0)[2],\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## LLM Output\")\n",
    "            llm_output = gr.TextArea(\n",
    "                label=\"LLM Output\",\n",
    "                value=\"LLM output will appear here...\",\n",
    "                lines=10,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                approve_btn = gr.Button(\"Approve and Save\")\n",
    "                save_status = gr.Markdown(\"\")\n",
    "\n",
    "    # ‚úÖ Connect LLM Generation to Button\n",
    "    run_prompt_btn.click(\n",
    "        generate_llm_response,  # Calls function\n",
    "        inputs=[index_slider, prompt_llm_text],  # Uses index & prompt template\n",
    "        outputs=[llm_output],  # Updates the LLM output field\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Update Interface When Slider Changes\n",
    "    index_slider.change(\n",
    "        show_data_point,\n",
    "        inputs=[index_slider],\n",
    "        outputs=[header, input_text, output_text],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
