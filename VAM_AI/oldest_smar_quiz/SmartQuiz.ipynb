{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murilofarias10/Python/blob/main/SmartQuiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FepAIY6qbeot"
      },
      "source": [
        "SmartQuiz with AI\n",
        "By Murilo Farias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhR3lv-3bhBh"
      },
      "source": [
        "Introduction\n",
        "This application empowers users to upload text (.txt) files and transform the content into interactive fill-in-the-blank quiz questions.\n",
        "Designed to merge technology and education, the tool dynamically generates questions based on the input text.\n",
        "It is ideal for educators, students, and self-learners.\n",
        "Using the AI-driven text processing and a user-friendly interface to ensure the explanation for quiz content with a good experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EylApU0bjMP"
      },
      "source": [
        "Step by Step\n",
        "Text Analysis: Analyzes uploaded content and selects key words for blank spaces.\n",
        "Dynamic Quiz Creation: Generates multiple-choice questions with randomized answer options.\n",
        "Interactive Feedback: Presents collapsible answers so users can learn immediately after attempting the question.\n",
        "AI Context Integration: Uses an AI agent to provide additional context or explanations for quiz content.\n",
        "\n",
        "1.   Hugging Face Agent Setup\n",
        "2.   Validate TXT File\n",
        "3.   Generate Quiz Questions\n",
        "4.   Generate IA Response\n",
        "5.   Process File and Generate Output\n",
        "6.   Create Gradio Interface\n",
        "7.   Launching the Application\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j7tTM5Hbggb",
        "outputId": "52a62c1e-abbd-4691-83e8-85ce117d0017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q smolagents==1.4.1 gradio==5.11.0 litellm==1.57.4\n",
        "from smolagents import MultiStepAgent, HfApiModel\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "import random\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -q smolagents==1.4.1 gradio==5.11.0 litellm==1.57.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "TOp4qq5Xb6mv",
        "outputId": "9910effe-7186-44e8-88e5-450e3bfb7b81"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Hugging Face token not found. Please set the HUGGINGFACE_TOKEN environment variable.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Configure the agent with placeholders\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MultiStepAgent(\n\u001b[32m     35\u001b[39m         tools=[],\n\u001b[32m     36\u001b[39m         verbosity_level=\u001b[32m0\u001b[39m,\n\u001b[32m     37\u001b[39m         model=model,\n\u001b[32m     38\u001b[39m         system_prompt=system_prompt + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m{{\u001b[39m\u001b[33mmanaged_agents_descriptions}}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m{{\u001b[39m\u001b[33mauthorized_imports}}\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m agent = \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# --- 2 Validate TXT File ---\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# It checks the file extension (only .txt is allowed).\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# It verifies that the file is not empty\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Content is extracted and prepared for analysis.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_txt_file\u001b[39m(file):\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcreate_agent\u001b[39m\u001b[34m(system_prompt)\u001b[39m\n\u001b[32m     20\u001b[39m token = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHUGGINGFACE_TOKEN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mHugging Face token not found. Please set the HUGGINGFACE_TOKEN environment variable.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m login(token)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#MODEL_ID = \"google/flan-t5-base\"\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: Hugging Face token not found. Please set the HUGGINGFACE_TOKEN environment variable."
          ]
        }
      ],
      "source": [
        "\n",
        "#!pip install -q smolagents==1.4.1 gradio==5.11.0 litellm==1.57.4\n",
        "\n",
        "from smolagents import MultiStepAgent, HfApiModel\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "import random\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# --- 1 Hugging Face Agent Setup ---\n",
        "# Authenticate to Hugging Face\n",
        "# Load the pre-trained AI model\n",
        "# Integrate the AI agent into the application workflow\n",
        "def create_agent(system_prompt=\"You are an intelligent agent that generates context for quiz questions.\"):\n",
        "    \"\"\"\n",
        "    Creates and returns a Hugging Face-based AI agent.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the Hugging Face token from the environment variable\n",
        "    token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        " \n",
        "\n",
        "    if not token:\n",
        "        raise ValueError(\"Hugging Face token not found. Please set the HUGGINGFACE_TOKEN environment variable.\")\n",
        "\n",
        "    login(token)\n",
        "\n",
        "    #MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    #MODEL_ID = \"google/flan-t5-base\"\n",
        "    MODEL_ID = \"tiiuae/falcon-7b-instruct\"\n",
        "    \n",
        "    model = HfApiModel(model_id=MODEL_ID)\n",
        "\n",
        "    # Configure the agent with placeholders\n",
        "    return MultiStepAgent(\n",
        "        tools=[],\n",
        "        verbosity_level=0,\n",
        "        model=model,\n",
        "        system_prompt=system_prompt + \"\\n{{managed_agents_descriptions}}\\n{{authorized_imports}}\"\n",
        "    )\n",
        "\n",
        "\n",
        "agent = create_agent()\n",
        "\n",
        "\n",
        "# --- 2 Validate TXT File ---\n",
        "# It checks the file extension (only .txt is allowed).\n",
        "# It verifies that the file is not empty\n",
        "# Content is extracted and prepared for analysis.\n",
        "def read_txt_file(file):\n",
        "    \"\"\"\n",
        "    Reads the uploaded TXT file and ensures it's valid.\n",
        "    Returns the content or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not file or not file.name.lower().endswith('.txt'):\n",
        "            return \"Error: Please upload a valid TXT file.\"\n",
        "        with open(file.name, 'r', encoding='utf-8') as f:\n",
        "            content = f.read().strip()\n",
        "        return content if content else \"Error: Empty file.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {str(e)}\"\n",
        "\n",
        "\n",
        "# --- 3 Generate Quiz Questions ---\n",
        "# Sentence Selection: Sentences are filtered to exclude short or irrelevant ones\n",
        "# Blank Creation: One word in the selected sentence is replaced with \"_____\".\n",
        "# Option Generation: The correct word and two similar but incorrect options are generated.\n",
        "# Shuffling: Options are shuffled to randomize their order.\n",
        "# Output: The formatted question, multiple-choice options, and collapsible correct answer are prepared for display.\n",
        "\n",
        "def create_quiz_questions(content):\n",
        "    \"\"\"\n",
        "    Processes the content and generates a single 'fill-in-the-blank' question.\n",
        "    Includes multiple-choice options and identifies the correct answer.\n",
        "    \"\"\"\n",
        "    if not content or isinstance(content, str) and content.startswith(\"Error\"):\n",
        "        return \"Could not generate question. Please check your file.\"\n",
        "\n",
        "    sentences = [s.strip() for s in content.split('\\n') if len(s.strip()) > 20 and ':' not in s]\n",
        "\n",
        "    if not sentences:\n",
        "        return \"Not enough content to generate a question.\"\n",
        "\n",
        "    all_words = set(word.lower() for sentence in sentences for word in sentence.split()\n",
        "                    if len(word) > 3 and not word.isupper())\n",
        "\n",
        "    if len(all_words) == 0:\n",
        "        return \"Could not generate words for options.\"\n",
        "\n",
        "    sentence = random.choice(sentences)\n",
        "\n",
        "    words = [word for word in sentence.split()\n",
        "             if len(word) > 3 and not word.isupper()\n",
        "             and word.lower() not in ['they', 'their', 'them', 'like', 'with', 'from']]\n",
        "\n",
        "    if not words:\n",
        "        return \"Could not generate a valid question from the content.\"\n",
        "\n",
        "    chosen_word = random.choice(words)\n",
        "    blank_sentence = sentence.replace(chosen_word, \"_____\", 1)\n",
        "\n",
        "    similar_words = [w for w in all_words\n",
        "                     if len(w) >= len(chosen_word) - 2 and len(w) <= len(chosen_word) + 2 and w != chosen_word.lower()]\n",
        "\n",
        "    if len(similar_words) < 2:\n",
        "        return \"Could not generate enough options.\"\n",
        "\n",
        "    wrong_options = random.sample(similar_words, 2)\n",
        "    options = [chosen_word] + wrong_options\n",
        "    random.shuffle(options)\n",
        "\n",
        "    complete_sentence = sentence.replace(chosen_word, chosen_word.capitalize(), 1)\n",
        "    return {\n",
        "        \"question\": blank_sentence,\n",
        "        \"options\": [f\"Option {chr(97 + i)}: {opt}\" for i, opt in enumerate(options)],\n",
        "        \"answer\": chosen_word,\n",
        "        \"correct_option\": f\"Option {chr(97 + options.index(chosen_word))}\",\n",
        "        \"complete_sentence\": complete_sentence\n",
        "    }\n",
        "\n",
        "\n",
        "# --- 4 Generate IA Response ---\n",
        "#Query AI Agent: Sends the complete sentence to the AI agent for additional context or explanation.\n",
        "#AI Response: Receives and displays the AI's response to the complete sentence.\n",
        "\n",
        "def generate_ia_response(complete_sentence):\n",
        "    \"\"\"\n",
        "    Queries the AI agent with the complete sentence and returns the IA's response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ia_response = agent.run(f\"Provide detailed information about: {complete_sentence}\")\n",
        "        return ia_response\n",
        "    except Exception as e:\n",
        "        return f\"Error processing IA response: {str(e)}\"\n",
        "\n",
        "# --- 5 Process File and Generate Output ---\n",
        "# File Processing: Reads the uploaded file and validates its content\n",
        "# Question Creation: Calls the function to generate a quiz question from the content.\n",
        "# Output Formatting: Prepares a clean and interactive Markdown format, including the question, multiple-choice options, and collapsible answers.\n",
        "# Hidden Storage: Stores the complete sentence internally for later use by the AI agent.\n",
        "def process_file(file):\n",
        "    \"\"\"\n",
        "    Reads the file, generates a question, and formats the output.\n",
        "    \"\"\"\n",
        "    content = read_txt_file(file)\n",
        "    if isinstance(content, str) and content.startswith(\"Error\"):\n",
        "        return content\n",
        "\n",
        "    question = create_quiz_questions(content)\n",
        "    if isinstance(question, str):\n",
        "        return question\n",
        "\n",
        "    # Updated formatting with better layout\n",
        "    output = \"### Fill in the blank:\\n\\n\"\n",
        "    output += f\"**{question['question']}**\\n\\n\"\n",
        "\n",
        "    output += \"**Choose the correct answer:**\\n\\n\"\n",
        "    for option in question['options']:\n",
        "        output += f\"* {option}\\n\"\n",
        "\n",
        "    output += f\"\\n<details><summary>👉 Show Answer</summary>\\n\\n\"\n",
        "    output += f\"**Correct answer: {question['answer']} ({question['correct_option']})**\\n\\n\"\n",
        "    output += f\"**Complete sentence:** {question['complete_sentence']}\\n\"\n",
        "    output += \"</details>\\n\"\n",
        "\n",
        "    # Return the complete sentence in a hidden format for the AI to use\n",
        "    return output, question['complete_sentence']\n",
        "\n",
        "def clean_outputs():\n",
        "    \"\"\"\n",
        "    Resets outputs to their initial state while preserving the file input.\n",
        "    \"\"\"\n",
        "    return gr.skip(), \"\", \"\"  # Skip file input reset, clear question output and AI response\n",
        "\n",
        "# --- 6 Create Gradio Interface ---\n",
        "#User Interface Design: Builds an interactive web interface using Gradio\n",
        "#Question and AI Response Display: Includes sections for displaying quiz questions and AI-provided context.\n",
        "# Button: Generate Question: Processes the file and generates a quiz question\n",
        "# Button: Get AI Response: Queries the AI agent for additional context.\n",
        "# Button: Clean: Resets the interface while preserving the file input.\n",
        "def create_interface():\n",
        "    \"\"\"\n",
        "    Builds the Gradio interface with custom styling for the quiz.\n",
        "    \"\"\"\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# 📚 SmartQuiz with AI by: Murilo Farias\")\n",
        "\n",
        "        with gr.Row():\n",
        "            file_input = gr.File(label=\"Upload TXT file\", file_types=[\".txt\"])\n",
        "            submit_btn = gr.Button(\"Generate Question\")\n",
        "            clean_btn = gr.Button(\"Clean\")  # New Clean button\n",
        "\n",
        "        output = gr.Markdown()\n",
        "        hidden_sentence = gr.Markdown(visible=False)  # Hidden storage for the complete sentence\n",
        "        ia_response = gr.Markdown(label=\"Generate AI Context\")  # Visible component for AI response\n",
        "\n",
        "        # Button click actions\n",
        "        submit_btn.click(fn=process_file, inputs=[file_input], outputs=[output, hidden_sentence])\n",
        "        ia_btn = gr.Button(\"Get AI Response\")\n",
        "        ia_btn.click(fn=generate_ia_response, inputs=[hidden_sentence], outputs=[ia_response])\n",
        "\n",
        "        # Clean button action - using gr.skip() to preserve file input\n",
        "        clean_btn.click(\n",
        "            fn=clean_outputs,\n",
        "            inputs=[],\n",
        "            outputs=[file_input, output, ia_response]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# --- 7 Launching the Application ---\n",
        "#App Initialization: Creates and configures the Gradio interface\n",
        "#Launching: Runs the Gradio interface and starts the application\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMc57qJn38Ae8g93zj24EwL",
      "include_colab_link": true,
      "mount_file_id": "1Ql6EEhphVDtEHzE5Q52VZY779X4rmg8R",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
